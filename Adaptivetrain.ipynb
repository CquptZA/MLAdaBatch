{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9cfa1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T13:38:22.554864Z",
     "start_time": "2024-03-24T13:38:21.223069Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from skmultilearn.dataset import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import random\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from skmultilearn.dataset import load_from_arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, f1_score, precision_score, recall_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sklearn.metrics as metrics\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import itertools\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pdb\n",
    "import scipy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "import math\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from util import *\n",
    "from layers import  *\n",
    "from model import *\n",
    "from function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c237c19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T13:38:22.573926Z",
     "start_time": "2024-03-24T13:38:22.557000Z"
    }
   },
   "outputs": [],
   "source": [
    "#####seed####\n",
    "def seed_all(seed): \n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "seed=42\n",
    "seed_all(seed)\n",
    "device = torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8699f700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T13:38:22.585438Z",
     "start_time": "2024-03-24T13:38:22.575903Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    def __init__(self,name,X,y):\n",
    "        self.name=name      \n",
    "        self.X_train=X\n",
    "        self.y_train=y\n",
    "        self.configs={}\n",
    "    \n",
    "    def getconfig(self):\n",
    "        self.configs['label_matrix']=np.array(self.y_train)\n",
    "        self.configs['num_classes']=self.y_train.shape[1] \n",
    "        self.configs['num_ins']=self.X_train.shape[0] \n",
    "        self.configs['seed']=42 \n",
    "        self.configs['batch_size']=128 \n",
    "        self.configs['epoch']=100 \n",
    "        self.configs['lr']=1e-4\n",
    "        self.configs['device']=torch.device(\"cuda:0\" ) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.configs['weight']=limb(np.array(self.X_train),np.array(self.y_train))\n",
    "        self.configs['extra_sample']=int(self.X_train.shape[0]*0.1)\n",
    "        self.configs['min_ins_idx']=minority_instance(np.array(self.y_train))\n",
    "        self.configs['minority_label_indices'],_=Labeltype(np.array(self.y_train))\n",
    "        self.configs['weight_list']=calweight(np.array(self.X_train),np.array(self.y_train))\n",
    "        #DELA\n",
    "        if self.name=='DELA':\n",
    "            self.configs['in_features']=self.X_train.shape[1] \n",
    "            self.configs['latent_dim']=math.ceil(self.X_train.shape[1]/2)    \n",
    "            self.configs['lr_ratio']=0.8\n",
    "            self.configs['drop_ratio']=0.2\n",
    "            self.configs['tau']=2/3\n",
    "            self.configs['beta']=1e-4\n",
    "            self.configs['out_index']=-1\n",
    "        if self.name=='CLIF':\n",
    "            self.configs['class_emb_size']=self.y_train.shape[1]  \n",
    "            self.configs['input_x_size']=self.X_train.shape[1] \n",
    "            self.configs['num_layers']=2 \n",
    "            self.configs['in_layers']=3 \n",
    "            self.configs['hidden_list']=[math.ceil(self.y_train.shape[1]/2)]  \n",
    "            self.configs['out_index']=0        \n",
    "        if self.name=='PACA':\n",
    "            self.configs['drop_ratio']=0.1\n",
    "            self.configs['latent_dim']=math.ceil(self.X_train.shape[1]/2)\n",
    "            self.configs['in_features']=self.X_train.shape[1] #输入x的维度\n",
    "            self.configs['rand_seed']=self.configs['seed']\n",
    "            self.configs['eps']=1e-8\n",
    "            self.configs['out_index']=-2\n",
    "            self.configs['lr_scheduler']='fix'\n",
    "            self.configs['binary_data']=False\n",
    "            self.configs['weight_decay']=1e-5\n",
    "            self.configs['alpha']=2\n",
    "            self.configs['gamma']=10\n",
    "            self.configs['scheduler_warmup_epoch']=5\n",
    "            self.configs['scheduler_decay_epoch']=10\n",
    "            self.configs['scheduler_decay_rate']=1e-5            \n",
    "        return self.configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e67a79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T13:38:22.627669Z",
     "start_time": "2024-03-24T13:38:22.586801Z"
    },
    "code_folding": [
     2,
     13,
     22,
     38,
     47,
     53,
     67,
     69,
     71
    ]
   },
   "outputs": [],
   "source": [
    "seed_all(seed)\n",
    "device = torch.device('cuda')\n",
    "def FeatureSelect(X,p):\n",
    "    if p==1:\n",
    "        return X.toarray(),feature_names\n",
    "    else:\n",
    "        featurecount=int(X.shape[1]*p)\n",
    "        Selectfeatureindex=[x[0] for x in (sorted(enumerate(X.sum(axis=0).tolist()[0]),key=lambda x: x[1],reverse=True))][:featurecount]\n",
    "        Allfeatureindex=[i for i in range(X.shape[1])]\n",
    "        featureindex=[i for i in Allfeatureindex if i not in Selectfeatureindex]\n",
    "        new_x=np.delete(X.toarray(),featureindex,axis=1)\n",
    "        new_featurename=[feature_names[i] for i in Selectfeatureindex] \n",
    "        return new_x,new_featurename\n",
    "def LabelSelect(y):\n",
    "    b=[]\n",
    "    new_labelname=[i for i in label_names]\n",
    "    for i in range(y.shape[1]):\n",
    "        if y[:,i].sum()<=20:\n",
    "            b.append(i)\n",
    "            new_labelname.remove(label_names[i])\n",
    "    new_y=np.delete(y.toarray(),b,axis=1)\n",
    "    return new_y,new_labelname\n",
    "def macro_averaging_auc(Y, P, O):\n",
    "    n = (Y.shape[0] + O.shape[0]) // 2\n",
    "    l = (Y.shape[1] + O.shape[1]) // 2\n",
    "\n",
    "    p = np.zeros(l)\n",
    "    q = np.sum(Y, 0)\n",
    "\n",
    "    zero_column_count = np.sum(q == 0)\n",
    "#     print(f\"all zero for label: {zero_column_count}\")\n",
    "    r, c = np.nonzero(Y)\n",
    "    for i, j in zip(r, c):\n",
    "        p[j] += np.sum((Y[ : , j] < 0.5) * (O[ : , j] <= O[i, j]))\n",
    "\n",
    "    i = (q > 0) * (q < n)\n",
    "\n",
    "    return np.sum(p[i] / (q[i] * (n - q[i]))) / l\n",
    "def hamming_loss(Y, P, O):\n",
    "    n = (Y.shape[0] + P.shape[0]) // 2\n",
    "    l = (Y.shape[1] + P.shape[1]) // 2\n",
    "\n",
    "    s1 = np.sum(Y, 1)\n",
    "    s2 = np.sum(P, 1)\n",
    "    ss = np.sum(Y * P, 1)\n",
    "\n",
    "    return np.sum(s1 + s2 - 2 * ss) / (n * l)\n",
    "def one_error(Y, P, O):\n",
    "    n = (Y.shape[0] + O.shape[0]) // 2\n",
    "\n",
    "    i = np.argmax(O, 1)\n",
    "\n",
    "    return np.sum(1 - Y[range(n), i]) / n\n",
    "def ranking_loss(Y, P, O):\n",
    "    n = (Y.shape[0] + O.shape[0]) // 2\n",
    "    l = (Y.shape[1] + O.shape[1]) // 2\n",
    "\n",
    "    p = np.zeros(n)\n",
    "    q = np.sum(Y, 1)\n",
    "\n",
    "    r, c = np.nonzero(Y)\n",
    "    for i, j in zip(r, c): \n",
    "        p[i] += np.sum((Y[i, : ] < 0.5) * (O[i, : ] >= O[i, j]))\n",
    "\n",
    "    i = (q > 0) * (q < l)\n",
    "\n",
    "    return np.sum(p[i] / (q[i] * (l - q[i]))) / n\n",
    "def micro_f1(Y, P, O):\n",
    "    return f1_score(Y, P, average='micro')\n",
    "def macro_f1(Y, P, O):\n",
    "    return f1_score(Y, P, average='macro')\n",
    "def eval_metrics(mod, metrics, datasets, idx,batch_size,device):\n",
    "    res_dict = {}\n",
    "    mod.eval()\n",
    "    y_true_list = []\n",
    "    y_scores_list = []\n",
    "    test_dataloader = DataLoader(datasets, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    for x, y in test_dataloader:\n",
    "        _, y_pred=mod.predict(x)\n",
    "#         pdb.set_trace()\n",
    "        y_true_list.append(y.cpu().numpy())\n",
    "        y_scores_list.append(y_pred.cpu().numpy())\n",
    "    y_true = np.vstack(y_true_list)\n",
    "    y_prob = np.vstack(y_scores_list)\n",
    "    y_pred = np.round(y_prob).astype(int)\n",
    "    res_dict1 = {metric.__name__: metric(y_true, y_pred,y_prob) for metric in metrics}\n",
    "#         # Calculate metric.\n",
    "#         res_dict1 = {metric.__name__: metric(y_true, y_pred) for metric in metrics[:2]}\n",
    "#         res_dict2 = {metric.__name__: metric(y_true, y_prob) for metric in metrics[2:5]}\n",
    "#         res_dict1.update(res_dict2)\n",
    "#     res_dict[f'dataset_{ix}']=res_dict1\n",
    "#         res_dict[f'dataset_{ix}'] = {metric.__name__: metric(y_true, y_pred) for metric in metrics[:8]}\n",
    "#         res_dict[f'dataset_{ix}'] = {metric.__name__: metric(y_true, y_prob) for metric in metrics[8:9]}\n",
    "    return res_dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94dbdad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T13:38:22.865799Z",
     "start_time": "2024-03-24T13:38:22.853236Z"
    },
    "code_folding": [
     0,
     12
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_probabilities(losses_dict,se):\n",
    "    losses_np = np.array(list(losses_dict.values()))\n",
    "#     std_dev = np.std(losses_np)\n",
    "#     Delta = std_dev\n",
    "    Delta=max(losses_np)/len(losses_np)\n",
    "    se = se \n",
    "    N = len(losses_np)\n",
    "    quantized_indices = np.ceil(losses_np / Delta).astype(int)\n",
    "    probabilities = (np.exp(np.log(se) / N)) ** quantized_indices\n",
    "    probabilities_dict = {key: prob for key, prob in zip(losses_dict.keys(), probabilities)}\n",
    "    \n",
    "    return probabilities_dict\n",
    "def get_top_keys_by_value(data_dict, top_n):\n",
    "    sorted_keys = sorted(data_dict, key=data_dict.get, reverse=True)[:top_n]\n",
    "    return sorted_keys\n",
    "class CustomDataLoader:\n",
    "    def __init__(self, dataset, net, configs,sample_probabilities,warm_epoch):\n",
    "        self.shuffle = False\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = configs['batch_size']\n",
    "        self.out_index=configs['out_index']\n",
    "        self.net = net\n",
    "        self.sample_probabilities =sample_probabilities\n",
    "        self.dataset_indices=list(range(len(self.dataset)))\n",
    "\n",
    "        self.warm_epoch=warm_epoch\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.current_epoch < self.warm_epoch:\n",
    "            batch_counter = 0\n",
    "            for start_idx in range(0, len(self.dataset_indices), self.batch_size):\n",
    "                end_idx = min(start_idx + self.batch_size, len(self.dataset_indices))\n",
    "                batch_indices = self.dataset_indices[start_idx:end_idx]\n",
    "                batch_counter += 1\n",
    "                yield self.get_data_from_indices(batch_indices)\n",
    "        else:\n",
    "            all_indices = list(self.sample_probabilities.keys())        \n",
    "            all_probabilities = np.array([self.sample_probabilities[idx] for idx in all_indices])\n",
    "            non_zero_mask = all_probabilities != 0\n",
    "            non_zero_sum = all_probabilities[non_zero_mask].sum()\n",
    "            all_probabilities[non_zero_mask] /= non_zero_sum\n",
    "            total_samples = len(self.dataset)\n",
    "            num_batches = np.ceil(total_samples / self.batch_size).astype(int)\n",
    "\n",
    "            for _ in range(num_batches):\n",
    "                num_required = self.batch_size \n",
    "\n",
    "                if num_required > 0:\n",
    "                    chosen_indices = np.random.choice(all_indices, size=num_required, replace=False, p=all_probabilities)\n",
    "                else:\n",
    "                    chosen_indices = []\n",
    "                batch_indices = list(chosen_indices)\n",
    "                yield self.get_data_from_indices(batch_indices)\n",
    "    def get_data_from_indices(self, indices):\n",
    "        x, y = zip(*[self.dataset[i] for i in indices])\n",
    "        return indices, torch.stack(x), torch.stack(y)\n",
    "    def set_epoch(self, epoch):\n",
    "        self.current_epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "870e4e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T13:39:40.730432Z",
     "start_time": "2024-03-24T13:39:40.717159Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def training(configs,warm_epoch,se):\n",
    "    net=DELA(configs).to(device)\n",
    "    num_epochs =configs['epoch']\n",
    "    batch_size = configs['batch_size']\n",
    "    lr = 1e-4\n",
    "    label_dim=configs['num_classes']\n",
    "    weight_decay = 1e-4  \n",
    "    betas = (0.9, 0.999) \n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr,  \n",
    "                           betas=betas, weight_decay=weight_decay)\n",
    "    writer = SummaryWriter(comment=f'{dataname}')\n",
    "\n",
    "    sample_probabilities = {}\n",
    "    para_loss={key: 0 for key in range(configs['num_ins'])}\n",
    "    custom_dataloader = CustomDataLoader(train_dataset, net=net, configs=configs,sample_probabilities=sample_probabilities,warm_epoch=warm_epoch)\n",
    "    validation_dataset, test_dataset_new = train_test_split(test_dataset, test_size=0.5, random_state=42)   \n",
    "    test_dataloader = DataLoader(test_dataset_new, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    best_auc = 0\n",
    "    best_model_state = None\n",
    "    epoch_losses_train = []\n",
    "\n",
    "    warmup_epochs =warm_epoch\n",
    "    total_steps = num_epochs * int(configs['num_ins']/batch_size)\n",
    "    warmup_steps = warmup_epochs * int(configs['num_ins']/batch_size)\n",
    "    global_step = 0\n",
    "    def update_learning_rate(optimizer, global_step, warmup_steps=warmup_steps, base_lr=lr):\n",
    "        if global_step < warmup_steps:\n",
    "            lr = base_lr * (global_step / warmup_steps)\n",
    "        else:\n",
    "            lr = base_lr\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr         \n",
    "    for epoch in range(num_epochs+1): \n",
    "        net.train()\n",
    "        batch_counter = 0   \n",
    "        loss_tracker = 0.0\n",
    "        loss_tracker2=0.0\n",
    "        all_individual_losses = {}\n",
    "        custom_dataloader.set_epoch(epoch) \n",
    "        for idx, x, y in custom_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(x)\n",
    "            loss_dict = net.loss_function_train(outputs, y)\n",
    "            loss = loss_dict['Loss']\n",
    "            individual_losses = net._loss_per_label(outputs[configs['out_index']], y)\n",
    "#             individual_losses =net.custom_multilabel_soft_margin_loss(outputs[configs['out_index']], y,  [configs['weight_list'][i] for i in idx])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            update_learning_rate(optimizer, global_step)\n",
    "\n",
    "            for i, prob in zip(idx, individual_losses):\n",
    "                para_loss[i]=prob.item()\n",
    "            current_probabilities = calculate_probabilities(para_loss,se)\n",
    "            for i in idx:\n",
    "                if i in current_probabilities:\n",
    "                    sample_probabilities[i] = current_probabilities[i]\n",
    "\n",
    "            custom_dataloader.sample_probabilities = sample_probabilities\n",
    "            batch_counter+=1\n",
    "        writer.add_scalar('train/loss', loss_tracker, epoch)\n",
    "        epoch_losses_train.append(loss_tracker /batch_counter)\n",
    "\n",
    "        net.eval()\n",
    "        y_true_list = []\n",
    "        y_scores_list = []\n",
    "        loss_tracker = 0.0\n",
    "        batch_counter = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in validation_dataloader:\n",
    "                _, y_pred=net.predict(x)\n",
    "                y_true_list.append(y.cpu().numpy())\n",
    "                y_scores_list.append(y_pred.cpu().numpy())\n",
    "                \n",
    "                outputs = net(x)\n",
    "                loss_dict = net.loss_function_train(outputs, y)\n",
    "                loss = loss_dict['Loss']\n",
    "                loss_tracker += loss.item()\n",
    "                batch_counter += 1\n",
    "        y_true = np.vstack(y_true_list)\n",
    "        y_scores = np.vstack(y_scores_list)\n",
    "\n",
    "        auc = macro_averaging_auc(y_true,y_scores, y_scores)\n",
    "#         writer.add_scalar('val/auc', auc, epoch)\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_epoch=epoch\n",
    "            best_model_state = net.state_dict().copy()\n",
    "    net.load_state_dict(best_model_state)\n",
    "    mets = eval_metrics(net, [macro_f1, micro_f1, macro_averaging_auc, ranking_loss, hamming_loss, one_error], test_dataset_new, configs['out_index'],configs['batch_size'],torch.device('cuda'))\n",
    "\n",
    "    return mets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c41c0b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T13:39:54.552582Z",
     "start_time": "2024-03-24T13:39:41.274439Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotions\n",
      "{'macro_f1': {'average': 0.6984, 'std': 0.0133}, 'micro_f1': {'average': 0.7177, 'std': 0.0136}, 'macro_averaging_auc': {'average': 0.8791, 'std': 0.0223}, 'ranking_loss': {'average': 0.1314, 'std': 0.0261}, 'hamming_loss': {'average': 0.1752, 'std': 0.0103}, 'one_error': {'average': 0.2154, 'std': 0.0514}}\n"
     ]
    }
   ],
   "source": [
    "path_to_arff_files = [\"emotions\",\"scene\",\"yeast\", \"Corel5k\",\"rcv1subset1\",\"rcv1subset2\",\"rcv1subset3\",\"yahoo-Business1\",\"yahoo-Arts1\",\"bibtex\",'tmc2007','enron','cal500','LLOG-F']\n",
    "label_counts = [6, 6,14,374,101,101,101,28,25,159,22,53,174,75]\n",
    "select_feature=[1,1,1,1,0.02,0.02,0.02,0.05,0.05,1,0.01,1,1,1]\n",
    "\n",
    "path_to_arff_files = [\"emotions\"]\n",
    "label_counts = [6]\n",
    "select_feature=[1]\n",
    "\n",
    "for idx, dataname in enumerate(path_to_arff_files):\n",
    "    path_to_arff_file = f\"/home/tt/{dataname}.arff\"\n",
    "    X, y, feature_names, label_names = load_from_arff(\n",
    "    path_to_arff_file,\n",
    "    label_count=label_counts[idx],\n",
    "    label_location=\"end\",\n",
    "    load_sparse=False,\n",
    "    return_attribute_definitions=True\n",
    "    )\n",
    "    X,feature_names=FeatureSelect(X,select_feature[idx])  \n",
    "    y,label_names=LabelSelect(y)\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    warm_epoch=3\n",
    "    print(dataname)\n",
    "    k_fold = IterativeStratification(n_splits=5,order=1,random_state=42)\n",
    "    dicts=[]\n",
    "    for idx,(train,test) in enumerate(k_fold.split(X,y)):\n",
    "        train_dataset = TensorDataset(torch.tensor(X[train], device=device, dtype=torch.float),torch.tensor(y[train], device=device,dtype=torch.float))\n",
    "        test_dataset = TensorDataset(torch.tensor(X[test], device=device, dtype=torch.float), torch.tensor(y[test], device=device, dtype=torch.float))    \n",
    "        configs=CFG('DELA',X[train],y[train]).getconfig()\n",
    "        dict_1=training(configs,warm_epoch,2)\n",
    "        dicts.append(dict_1)\n",
    "    averages_and_stds = {}\n",
    "    for key in dicts[0].keys():\n",
    "        values = [d[key] for d in dicts]\n",
    "        averages_and_stds[key] = {\n",
    "            'average': round(np.mean(values),4),\n",
    "            'std': round(np.std(values),4)\n",
    "        }\n",
    "    print(averages_and_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee3a720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
